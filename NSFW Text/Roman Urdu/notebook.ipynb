{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import fasttext\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import unicodedata\n",
    "from emoji import demojize\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaning(text_data):\n",
    "\n",
    "  # Remove accented characters\n",
    "  text_data = unicodedata.normalize('NFKD', text_data).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "\n",
    "  # Case conversion\n",
    "  text_data = text_data.lower()\n",
    "\n",
    "  # Demojize\n",
    "  text_data = demojize(text_data)\n",
    "\n",
    "  # Reducing repeated punctuations\n",
    "  pattern_punct = re.compile(r'([.,/#!$%^&*?;:{}=_`~()+-])\\1{1,}')\n",
    "  text_data = pattern_punct.sub(r'\\1', text_data)\n",
    "  \n",
    "  # Prevent redundant replacements of single-space with single-space\n",
    "  text_data = re.sub(' {2,}',' ', text_data)\n",
    "  \n",
    "  # Remove special characters\n",
    "  text_data = re.sub(r\"[^a-zA-Z?!]+\", ' ', text_data)\n",
    "\n",
    "  # Converting text to strings\n",
    "  text_data = str(text_data)\n",
    "\n",
    "  # Tokenization\n",
    "  tokenizer = ToktokTokenizer()\n",
    "  text_data = tokenizer.tokenize(text_data)\n",
    "\n",
    "  # Removing stopwords\n",
    "  text_data = [item for item in text_data if item not in stop_words]\n",
    "  \n",
    "  # Lemmatization\n",
    "  text_data = [lemmatizer.lemmatize(word = w, pos = 'v') for w in text_data]\n",
    "  \n",
    "  # Convert list of tokens to string data type\n",
    "  text_data = ' '.join (text_data)\n",
    "\n",
    "  return text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.train_supervised('train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model(\"profanity_model_urdu.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import fasttext\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import unicodedata\n",
    "from emoji import demojize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\CC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaning(text_data):\n",
    "\n",
    "  # Remove accented characters\n",
    "  text_data = unicodedata.normalize('NFKD', text_data).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "\n",
    "  # Case conversion\n",
    "  text_data = text_data.lower()\n",
    "\n",
    "  # Demojize\n",
    "  text_data = demojize(text_data)\n",
    "\n",
    "  # Reducing repeated punctuations\n",
    "  pattern_punct = re.compile(r'([.,/#!$%^&*?;:{}=_`~()+-])\\1{1,}')\n",
    "  text_data = pattern_punct.sub(r'\\1', text_data)\n",
    "  \n",
    "  # Prevent redundant replacements of single-space with single-space\n",
    "  text_data = re.sub(' {2,}',' ', text_data)\n",
    "  \n",
    "  # Remove special characters\n",
    "  text_data = re.sub(r\"[^a-zA-Z?!]+\", ' ', text_data)\n",
    "\n",
    "  # Converting text to strings\n",
    "  text_data = str(text_data)\n",
    "\n",
    "  # Tokenization\n",
    "  tokenizer = ToktokTokenizer()\n",
    "  text_data = tokenizer.tokenize(text_data)\n",
    "\n",
    "  # Removing stopwords\n",
    "  text_data = [item for item in text_data if item not in stop_words]\n",
    "  \n",
    "  # Lemmatization\n",
    "  text_data = [lemmatizer.lemmatize(word = w, pos = 'v') for w in text_data]\n",
    "  \n",
    "  # Convert list of tokens to string data type\n",
    "  text_data = ' '.join (text_data)\n",
    "\n",
    "  return text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.load_model('profanity_model_urdu(2).bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\CC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input: tum ghalat ho\n",
      "NSFW: 97.7%\n",
      " SFW: 2.3%\n",
      "\n",
      "Input: ghalat\n",
      "NSFW: 97.2%\n",
      " SFW: 2.8%\n",
      "\n",
      "Input: stop\n",
      " SFW: 100.0%\n",
      "NSFW: 0.0%\n"
     ]
    }
   ],
   "source": [
    "user_input = \"\"\n",
    "while user_input != \"stop\":\n",
    "    user_input = input(\"Enter something: \")\n",
    "    print(\"\\nInput:\", user_input)\n",
    "    user_input = text_cleaning(user_input)\n",
    "\n",
    "    labels, probabilities = model.predict(user_input, k=2)\n",
    "\n",
    "    for label, probability in zip(labels, probabilities):\n",
    "        if label[9:] == \"1\":\n",
    "            print(f'NSFW: {round(probability*100, 1)}%')\n",
    "        else:\n",
    "            print(f' SFW: {round(probability*100, 1)}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
