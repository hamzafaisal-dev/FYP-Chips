{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import fasttext\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import unicodedata\n",
    "from emoji import demojize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaning(text_data):\n",
    "\n",
    "  # Remove accented characters\n",
    "  text_data = unicodedata.normalize('NFKD', text_data).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "\n",
    "  # Case conversion\n",
    "  text_data = text_data.lower()\n",
    "\n",
    "  # Demojize\n",
    "  text_data = demojize(text_data)\n",
    "\n",
    "  # Reducing repeated punctuations\n",
    "  pattern_punct = re.compile(r'([.,/#!$%^&*?;:{}=_`~()+-])\\1{1,}')\n",
    "  text_data = pattern_punct.sub(r'\\1', text_data)\n",
    "  \n",
    "  # Prevent redundant replacements of single-space with single-space\n",
    "  text_data = re.sub(' {2,}',' ', text_data)\n",
    "  \n",
    "  # Remove special characters\n",
    "  text_data = re.sub(r\"[^a-zA-Z?!]+\", ' ', text_data)\n",
    "\n",
    "  # Converting text to strings\n",
    "  text_data = str(text_data)\n",
    "\n",
    "  # Tokenization\n",
    "  tokenizer = ToktokTokenizer()\n",
    "  text_data = tokenizer.tokenize(text_data)\n",
    "\n",
    "  # Removing stopwords\n",
    "  text_data = [item for item in text_data if item not in stop_words]\n",
    "  \n",
    "  # Lemmatization\n",
    "  text_data = [lemmatizer.lemmatize(word = w, pos = 'v') for w in text_data]\n",
    "  \n",
    "  # Convert list of tokens to string data type\n",
    "  text_data = ' '.join (text_data)\n",
    "\n",
    "  return text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.load_model('profanity_model_eng.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"\"\n",
    "while user_input != \"stop\":\n",
    "    user_input = input(\"Enter something: \")\n",
    "    print(\"\\nYou entered:\", user_input)\n",
    "    user_input = text_cleaning(user_input)\n",
    "\n",
    "    labels, probabilities = model.predict(user_input, k=2)\n",
    "\n",
    "    for label, probability in zip(labels, probabilities):\n",
    "        if label[9:] == \"1\":\n",
    "            print(f'Profane: {round(probability*100, 1)}%')\n",
    "        else:\n",
    "            print(f'Clean: {round(probability*100, 1)}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
